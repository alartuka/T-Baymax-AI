import { NextResponse } from "next/server";
import dotenv from 'dotenv';
dotenv.config();


// System prompt for the AI, providing guidelines on how to respond to users
const systemPrompt = `
You are a Healthcare and Personal Companion called "T-Baymax". 
Your primary function is to assist users with both healthcare-related inquiries and general life support. 
You act as a personal healthcare companion and lifestyle or wellness advisor, 
providing accurate guidance on health-related topics, wellness tips, product inquiries, troubleshooting, and support requests.

Your target audience includes individuals seeking healthcare guidance and general consumers. 
You are designed for those looking for empathetic, accurate, informative interactions that cater to their health 
and wellness needs, as well as inquiries about other aspects of their life.
You understand and respond accurately to a wide range of healthcare and life advice queries in a natural, 
conversational manner, ensuring positive experience for users.

Your capabilities allow you to detect emotional and health-related cues in users' language, 
providing empathetic and reassuring responses to ensure they feel understood and supported, 
whether discussing health concerns or product issues. 
You are available at all times, offering constant support for health-related inquiries, wellness guidance, 
and inquiries about other life aspects, ensuring users can access assistance whenever they need it.

You support multiple languages, allowing users from different regions to receive healthcare and 
support related to other aspects of life in their preferred language. 
You can provide proactive health and wellness suggestions, such as reminders to stay hydrated, exercise, or take medications. 
You also offer preventative health tips based on user interactions.

Your tone is warm, friendly, and reassuring, while remaining knowledgeable and authoritative on health-related topics. 
You communicate in a clear, jargon-free manner to ensure users feel safe and well-informed.
`


// Initialize the OpenAI API with your API key
const { GoogleGenerativeAI } = require("@google/generative-ai");

let genAI;
if (process.env.NEXT_PUBLIC_GEMINI_API_KEY) {
  genAI = new GoogleGenerativeAI(process.env.NEXT_PUBLIC_GEMINI_API_KEY);
} else {
    throw new Error("Please provide API KEY env variable!")
}

const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash",   systemInstruction: systemPrompt})

export async function POST(req) { 
    
    try {
        const { messages } = await req.json()
        console.log(messages)
        const prompt = messages[messages.length - 1].content;
        const result = await model.generateContent(prompt)
        return NextResponse.json(result.response.text(), { status: 200 });
        // return new NextResponse(result.response.text(), { status: 200 })
    } catch (err) {
        console.error("Error processing request:", err);
        return new NextResponse("Internal Server Error", { status: 500 });
    }
}




// const openai = new OpenAI({
//     baseURL: process.env.OPENROUTER_API_URL,
//     apiKey: process.env.OPENROUTER_API_KEY,
//   });



// // POST function to handle incoming requests
// export async function POST(req) {
//     const data = await req.json() // Parse the JSON body of the incoming request

//     // Make a POST request to the OpenRouter API
//     const response = await fetch('https://openrouter.ai/api/v1/chat/completions',
//         {
//             method: 'POST',
//             headers: {
//                 'Content-Type': 'application/json',
//                 'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`
//             },
//             body: JSON.stringify({
//                 model: 'meta-llama/llama-3.1-8b-instruct:free',
//                 messages: [{ role: "system", content: systemPrompt}, ...data],
//             }),

//         }
//     );
//     const result = await response.json();

//     // Extract content from the response
//     const content = result.choices[0]?.message.content || 'No response recieved';
//     // Return the response as a JSON object/return the stream as a response
//     return new NextResponse(content);
// }


// POST function to handle incoming requests
// export async function POST(req) {
    
//     const data = await req.json() // Parse the JSON body of the incoming request
    
//     try {
//         // const response = await fetch()
//     // Create a chat completion request to the API
//     const completion = await openai.chat.completions.create({
//         model:"meta-llama/llama-3.1-8b-instruct:free",
//         messages: [{ role: "system", content: systemPrompt}, ...data],
//         // stream: true,
//     })

//     console.log('<API Response>', completion.choices[0].message.content)
//     res.status(200).json({ message: completion.choices[0]?.message.content });
//     } catch (error) {
//         console.error(`Error generating message response: ${error.message}`);
//         res.status(500).json({ error: 'Internal Server Error' });
//     }

//     // Create a ReadableStream to handle the streaming response
//     // const stream = new ReadableStream({
//     //     async start(controller) {
//     //         const encoder = new TextEncoder() // Create a TextEncoder to convert strings to Uint8Array
//     //         try {
//     //             // Iterate over the streamed chunks of the response
//     //             for await (const chunk of  completion) {
//     //                 const content = chunk.choices[0]?.delta?.content // Extract the content from the chunk
//     //                 if (content) {
//     //                     const text = encoder.encode(content)// Encode the content to Uint8Array
//     //                     controller.enqueue(text) // Enqueue the encoded text to the stream
//     //                 }
//     //             } 
//     //         } catch (err) {
//     //             console.error(err)// Handle any errors that occur during streaming
//     //         } finally {
//     //             controller.close() // Close the stream when done
//     //         }
//     //     },
//     // })
    
//     // return new NextResponse(stream) // Return the stream as the response
// }

